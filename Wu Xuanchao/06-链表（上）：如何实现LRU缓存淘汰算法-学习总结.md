# 06-链表（上）：如何实现LRU缓存淘汰算法-学习总结

LRU 缓存淘汰算法是链表的一个经典应用场景。缓存是一种提高数据读取性能的技术，当缓存被用满时，需要缓存淘汰策略来决定数据的去留。常见的策略有三种：

- 先进先出策略 FIFO（First In，First Out）
- 最少使用策略 LFU（Least Frequently Used）
- 最近最少使用策略 LRU（Least Recently Used）

## 五花八门的链表结构

数组和链表的区别：

从底层的存储结构上来看，数组需要一块连续的内存空间来存储，链表可以通过指针将一组零散的内存块串联起来使用。

三种最常见的链表结构：

- 单链表
- 双向链表
- 循环链表

### 单链表

链表通过指针将一组零散的内存块串联在一起，其中的内存块称为链表的**结点**。为了将所有结点串起来，每个链表结点不仅要存储数据，还需要记录链上下一个结点的地址，这个地址的指针叫作**后继指针 next**。链表的第一个结点叫作**头结点**，它用来记录链表的基地址，最后一个结点叫作**尾结点**，尾结点的指针指向一个空地址 NULL。

针对链表的**插入和删除**操作，不需要像数组一样搬移数据，因为链表的存储空间本身就是**不连续**的，只需要考虑相邻结点的指针改变，所以对应的时间复杂度是 O(1)。但有利就有弊，因为链表中的数据并非连续存储，所以无法像数组那样根据首地址和下标，通过寻址公式直接计算出对应的内存地址，而是需要根据指针逐个结点依次遍历，所以链表的**随机访问**需要 O(n) 的时间复杂度。

### 循环链表

循环链表是一种特殊的单链表，它跟单链表的唯一区别是，它的尾结点指针指向链表的头结点，像环一样首尾相连，所以它的优点是从链尾到链头比较方便。

### 双向链表

双向链表的每个每个结点不止有一个后继指针，还有一个**前驱指针 prev 指向前面的结点**，所以它支持在 O(1) 时间复杂度的情况下找到前驱结点。因为这个特点，**双向链表在某些情况下的插入、删除等操作要比单链表高效**，以下是具体分析。

从链表中删除一个数据通常有两种情况：

- 删除结点中「值等于某个给定值」的结点；
- 删除给定指针指向的结点。

对于第一种情况，单链表和双向链表为了查找等于给定值的结点，都需要从头结点开始遍历，因此时间复杂度都为 O(n)。

对于第二种情况，删除给定指针的结点需要知道其前驱结点，单链表还是要从头结点遍历，而双向链表直接可以获取前驱结点的指针。同理，在某个指定结点前面插入一个结点，双向链表也比单链表高效。

除了插入、删除操作有优势外，对于一个有序链表，双向链表的按值查询效率也比单链表高，因为可以根据要查找的值与上次查找的位置关系，决定查找的方向是往前还是往后，这样平均只需要查找一半的数据。

虽然双向链表比较费内存，但它比单链表更高效，应用也更广泛。双向链表体现了**用空间换时间**的设计思想，同样的，缓存实际上也是利用了这个设计思想。对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗。

### 双向循环链表

双向循环链表就是把循环链表和双向链表整合在一起。

## 链表 VS 数组性能大比拼

因为内存存储的区别，数组和链表在时间复杂度上的区别如下：

| 时间复杂度 | 数组 | 链表 |
| ---------- | ---- | ---- |
| 插入、删除 | O(n) | O(1) |
| 随机访问   | O(1) | O(n) |

数组和链表在性能上还有其他方面的区别：

1. 在利用 CPU 缓存机制上，因为使用连续的内存空间，所以可以借助 CPU 缓存机制预读数据，而链表则不行。

2. 在大小和扩容方面，数组大小固定，一经声明就要占用整块内存空间，容易导致内存不足（out of memory），扩容时需要复制原数组会非常耗时；链表没有大小限制，天然支持动态扩容。

3. 在内存使用方面，数组更为友好，因为链表需要存储指针，所以内存消耗大，而且对链表进行频繁的插入、删除操作，还会导致内存的申请和释放，这样容易造成内存碎片，以及频繁的垃圾回收。

可以说，链表是一种跟数组「相反」的数据结构。

## 解答开篇

如何基于链表实现 LRU 缓存淘汰算法？

我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。

1. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。
2. 如果此数据没有在缓存链表中，又可以分为两种情况：
   - 如果此时缓存未满，则将此结点直接插入到链表的头部；
   - 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。

这样我们就用链表实现了一个 LRU 缓存。

现在我们来看下 m 缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为 O(n)。

实际上，我们可以继续优化这个实现思路，比如引入**散列表**（Hash table）来记录每个数据的位置，将缓存访问的时间复杂度降到 O(1)。

## 课后思考

如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？

1. 快慢指针定位中间节点
2. 从中间节点对后半部分逆序
3. 前后半部分比较，判断是否为回文
4. 后半部分逆序复原

时间复杂度 On，空间复杂度 O1。

## 其他问题

> 数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。” 这里的CPU缓存机制指的是什么？为什么就数组更好了？

CPU在从内存读取数据的时候，会先把读取到的数据加载到CPU的缓存中。而CPU每次从内存读取数据并不是只读取那个特定要访问的地址，而是读取一个数据块(这个大小我不太确定。。)并保存到CPU缓存中，然后下次访问内存数据的时候就会先从CPU缓存开始查找，如果找到就不需要再从内存中取。这样就实现了比内存访问速度更快的机制，也就是CPU缓存存在的意义:为了弥补内存访问速度过慢与CPU执行速度快之间的差异而引入。

对于数组来说，存储空间是连续的，所以在加载某个下标的时候可以把以后的几个下标元素也加载到CPU缓存这样执行速度会快于存储空间不连续的链表存储。  

